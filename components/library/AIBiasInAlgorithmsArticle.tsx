import React from 'react';
import { ScrollView, Text, StyleSheet, Image, TouchableOpacity, View, Dimensions } from 'react-native';
import { useRouter } from 'expo-router';
import { AntDesign } from '@expo/vector-icons';

const { width: SCREEN_WIDTH } = Dimensions.get('window');

const AIBiasInAlgorithmsArticle = () => {
  const router = useRouter();

  return (
    <ScrollView contentContainerStyle={styles.container}>
      <TouchableOpacity style={styles.backButton} onPress={() => router.back()}>
        <AntDesign name="arrowright" size={22} color="#333" />
      </TouchableOpacity>

      <View style={styles.contentWrapper}>
        <Text style={styles.title}>بحث: التحيز في خوارزميات الذكاء الاصطناعي</Text>
        <Image
          source={require('../../assets/images/AIBiasInAlgorithmsArticle.jpg')}
          style={styles.image}
          resizeMode="cover"
        />

        <Text style={styles.sectionTitle}>المقدمة</Text>
        <Text style={styles.content}>
          التحيز في خوارزميات الذكاء الاصطناعي هو قضية أخلاقية وتقنية تؤثر بشكل مباشر على مصداقية وفعالية الأنظمة الذكية. يهدف هذا البحث إلى مناقشة مصادر التحيز، تأثيره على القرارات، والطرق الممكنة للحد منه من خلال تحسين البيانات والخوارزميات، مع استعراض دراسات حالة واقعية.
          {"\n\n"}
          مع تزايد الاعتماد على الذكاء الاصطناعي في مختلف مجالات الحياة، أصبح من الضروري دراسة التحيزات الكامنة في هذه الأنظمة، وفهم كيف يمكن أن تؤثر على الأفراد والمجتمعات، خاصة في القطاعات الحساسة مثل الصحة، العدالة، التوظيف، والتعليم.
          {"\n\n"}
          لا يقتصر التحيز على الجانب التقني فقط، بل يمتد ليشمل أبعادًا اجتماعية وثقافية واقتصادية، حيث يمكن أن يؤدي إلى تعزيز الفوارق القائمة أو خلق تحديات جديدة. إن فهم جذور التحيز في الذكاء الاصطناعي يتطلب تحليلًا متعدد التخصصات يجمع بين علوم الحاسوب، علم الاجتماع، الفلسفة، والقانون.
          {"\n\n"}
          تشير تقارير حديثة إلى أن أكثر من 60% من المؤسسات التي تعتمد على الذكاء الاصطناعي واجهت تحديات تتعلق بالتحيز في أنظمتها، مما دفع العديد من الشركات والحكومات إلى تبني سياسات جديدة للحد من هذه الظاهرة.
          {"\n\n"}
        </Text>

        <Text style={styles.sectionTitle}>1. تعريف التحيز في الذكاء الاصطناعي</Text>
        <Text style={styles.content}>
          التحيز في الذكاء الاصطناعي يشير إلى وجود انحرافات في نتائج الخوارزميات بسبب بيانات تدريب غير متوازنة أو تصميم خوارزمي غير محايد. يمكن أن يؤدي ذلك إلى قرارات غير عادلة أو تمييز ضد مجموعات معينة.
          {"\n\n"}
          من المهم التمييز بين التحيز المتعمد وغير المتعمد، حيث قد ينشأ التحيز أحيانًا نتيجة عوامل خارجية مثل نقص البيانات أو سوء تمثيل الفئات.
          {"\n\n"}
          هناك أنواع متعددة من التحيز، منها: التحيز الإحصائي، التحيز المعرفي، التحيز الاجتماعي، والتحيز الناتج عن التفاعل بين الإنسان والآلة. كل نوع له تأثيراته الخاصة على نتائج الأنظمة الذكية.
          {"\n\n"}
          <Text style={styles.subSectionTitle}>اقتباس:</Text>
          {"\n"}
          "التحيز في الذكاء الاصطناعي ليس خطأ برمجيًا فقط، بل هو انعكاس لتحيزات المجتمع نفسه."{"\n"}
          — د. ليلى السبيعي، باحثة في أخلاقيات الذكاء الاصطناعي
          {"\n\n"}
        </Text>

        <Text style={styles.sectionTitle}>2. مصادر التحيز</Text>
        <Text style={styles.content}>
          {"\u2022"} بيانات التدريب: إذا كانت البيانات غير ممثلة لجميع الفئات، سينعكس ذلك على أداء النموذج.
          {"\n"}
          {"\u2022"} تصميم الخوارزميات: قد تتضمن الخوارزميات افتراضات مسبقة تؤدي إلى التحيز.
          {"\n"}
          {"\u2022"} التحيز البشري: تدخل المبرمجين في اختيار البيانات أو المعايير.
          {"\n"}
          {"\u2022"} نقص الشفافية: صعوبة فهم كيفية اتخاذ الخوارزمية للقرار.
          {"\n"}
          {"\u2022"} التحيز الناتج عن البيئة: مثل تأثير الثقافة المحلية أو اللغة على البيانات.
          {"\n\n"}
          <Text style={styles.subSectionTitle}>جدول مقارنة بين مصادر التحيز:</Text>
          {"\n"}
          <Text>
            {"مصدر التحيز        | مثال عملي                | التأثير"}
            {"\n"}
            {"-------------------|--------------------------|----------------------"}
            {"\n"}
            {"البيانات            | صور غير متوازنة للأعراق  | نتائج غير دقيقة"}
            {"\n"}
            {"الخوارزمية          | افتراضات مسبقة           | تمييز ضمني"}
            {"\n"}
            {"الإنسان             | اختيار معايير غير عادلة  | تحيز معرفي"}
            {"\n"}
            {"البيئة              | لغة أو ثقافة معينة       | إقصاء فئات"}
          </Text>
          {"\n\n"}
        </Text>

        <Text style={styles.sectionTitle}>3. أمثلة واقعية على التحيز</Text>
        <Text style={styles.content}>
          {"\u2022"} أنظمة التوظيف: أظهرت بعض الخوارزميات تفضيلًا لجنس أو عرق معين، مما أدى إلى استبعاد مرشحين مؤهلين.
          {"\n"}
          {"\u2022"} التعرف على الوجوه: أداء أقل دقة مع بعض الأعراق أو الفئات العمرية، مما تسبب في أخطاء في التعرف على الهوية.
          {"\n"}
          {"\u2022"} القروض البنكية: قرارات غير عادلة في منح القروض بناءً على بيانات تاريخية متحيزة.
          {"\n"}
          {"\u2022"} أنظمة العدالة الجنائية: استخدام خوارزميات التنبؤ بالجريمة التي تعزز الصور النمطية السلبية.
          {"\n"}
          {"\u2022"} تطبيقات الرعاية الصحية: تشخيصات غير دقيقة لفئات سكانية معينة بسبب نقص تمثيلهم في بيانات التدريب.
          {"\n\n"}
          <Text style={styles.subSectionTitle}>دراسة حالة:</Text>
          {"\n"}
          في عام 2018، كشفت دراسة أجرتها MIT عن وجود تحيز كبير في أنظمة التعرف على الوجوه التجارية، حيث كانت معدلات الخطأ أعلى بكثير عند التعرف على النساء والأشخاص ذوي البشرة الداكنة مقارنة بالرجال ذوي البشرة الفاتحة.
          {"\n\n"}
          <Text style={styles.subSectionTitle}>مثال إضافي:</Text>
          {"\n"}
          في عام 2020، واجهت إحدى شركات التأمين انتقادات واسعة بعد اكتشاف أن خوارزميتها تمنح أسعار تأمين أعلى للأقليات، رغم تساوي عوامل الخطر، بسبب اعتمادها على بيانات تاريخية متحيزة.
          {"\n\n"}
        </Text>

        <Text style={styles.sectionTitle}>4. تأثير التحيز على المجتمع</Text>
        <Text style={styles.content}>
          يؤدي التحيز في الخوارزميات إلى فقدان الثقة في الأنظمة الذكية، ويؤثر سلبًا على العدالة الاجتماعية. قد يتسبب في تعزيز الفوارق الاجتماعية والاقتصادية، ويؤدي إلى اتخاذ قرارات مصيرية غير عادلة.
          {"\n\n"}
          <Text style={styles.subSectionTitle}>أمثلة على التأثيرات السلبية:</Text>
          {"\n"}
          {"\u2022"} تهميش فئات معينة من المجتمع.
          {"\n"}
          {"\u2022"} تعزيز الصور النمطية السلبية.
          {"\n"}
          {"\u2022"} فقدان فرص العمل أو التعليم بسبب قرارات آلية متحيزة.
          {"\n"}
          {"\u2022"} زيادة الفجوة الرقمية بين المجتمعات.
          {"\n"}
          {"\u2022"} التأثير على الصحة النفسية للأفراد المتضررين من قرارات غير عادلة.
          {"\n\n"}
          <Text style={styles.subSectionTitle}>تحليل اجتماعي:</Text>
          {"\n"}
          تشير الدراسات إلى أن المجتمعات التي تعتمد بشكل مفرط على الأنظمة الذكية دون رقابة بشرية معرضة لخطر ترسيخ التحيزات القائمة، مما يتطلب تدخلًا تشريعيًا وأخلاقيًا عاجلًا.
          {"\n\n"}
        </Text>

        <Text style={styles.sectionTitle}>5. طرق الحد من التحيز</Text>
        <Text style={styles.content}>
          {"\u2022"} جمع بيانات متنوعة وشاملة: التأكد من تمثيل جميع الفئات في بيانات التدريب.
          {"\n"}
          {"\u2022"} اختبار النماذج على مجموعات بيانات متعددة: للكشف عن التحيزات الخفية.
          {"\n"}
          {"\u2022"} مراجعة الخوارزميات من قبل خبراء مستقلين: لضمان الحياد والعدالة.
          {"\n"}
          {"\u2022"} تطبيق معايير أخلاقية صارمة في تطوير الذكاء الاصطناعي.
          {"\n"}
          {"\u2022"} زيادة الشفافية: توثيق كيفية اتخاذ القرارات وتوضيحها للمستخدمين.
          {"\n"}
          {"\u2022"} إشراك المجتمعات المتأثرة في عملية تطوير الأنظمة الذكية.
          {"\n"}
          {"\u2022"} استخدام تقنيات الذكاء الاصطناعي التوضيحي (Explainable AI) لفهم أسباب القرارات.
          {"\n\n"}
          <Text style={styles.subSectionTitle}>دراسة حالة:</Text>
          {"\n"}
          أطلقت شركة Google مبادرة "AI Fairness" لتطوير أدوات تساعد المطورين على اكتشاف وتصحيح التحيزات في نماذجهم.
          {"\n"}
          كما قامت شركة IBM بتطوير أداة "AI Fairness 360" التي توفر مكتبة مفتوحة المصدر لرصد وتصحيح التحيز في البيانات والنماذج.
          {"\n\n"}
        </Text>

        <Text style={styles.sectionTitle}>6. دراسات حالة وأبحاث حديثة</Text>
        <Text style={styles.content}>
          {"\u2022"} دراسة MIT حول التحيز في أنظمة التعرف على الوجوه: كشفت عن فروقات كبيرة في معدلات الخطأ بين الفئات المختلفة.
          {"\n"}
          {"\u2022"} تحليل خوارزميات التوظيف في شركات التكنولوجيا الكبرى: أظهرت وجود تحيزات ضد النساء والأقليات.
          {"\n"}
          {"\u2022"} مبادرات Google وMicrosoft للحد من التحيز في منتجاتهم: تطوير أدوات وتقنيات لرصد وتصحيح التحيزات.
          {"\n"}
          {"\u2022"} مشروع AI Now Institute: يركز على دراسة التأثيرات الاجتماعية والأخلاقية للذكاء الاصطناعي.
          {"\n"}
          {"\u2022"} دراسة حديثة في جامعة ستانفورد حول تأثير التحيز في تطبيقات الرعاية الصحية الذكية، وأهمية تمثيل جميع الفئات السكانية في بيانات التدريب.
          {"\n\n"}
          <Text style={styles.subSectionTitle}>مقارنة بين استراتيجيات الحد من التحيز:</Text>
          {"\n"}
          {"\u2022"} جمع بيانات جديدة مقابل تعديل الخوارزمية.
          {"\n"}
          {"\u2022"} مراجعة بشرية مقابل مراجعة آلية.
          {"\n"}
          {"\u2022"} استخدام الذكاء الاصطناعي التوضيحي مقابل الأنظمة السوداء (Black Box).
          {"\n\n"}
          <Text style={styles.subSectionTitle}>اقتباس بحثي:</Text>
          {"\n"}
          "إن تطوير أنظمة ذكاء اصطناعي عادلة يتطلب تضافر الجهود بين التقنيين وصناع السياسات والمجتمع المدني."{"\n"}
          — تقرير AI Now Institute، 2023
          {"\n\n"}
        </Text>

        <Text style={styles.sectionTitle}>7. مستقبل الذكاء الاصطناعي العادل</Text>
        <Text style={styles.content}>
          يتوقع الخبراء أن يشهد مجال الذكاء الاصطناعي العادل تطورًا كبيرًا في السنوات القادمة، مع التركيز على:
          {"\n"}
          {"\u2022"} تطوير أدوات آلية لرصد التحيز وتصحيحه أثناء التدريب.
          {"\n"}
          {"\u2022"} سن تشريعات وقوانين تنظم استخدام الذكاء الاصطناعي وتضمن العدالة.
          {"\n"}
          {"\u2022"} تعزيز التعاون بين الباحثين والمطورين وصناع القرار.
          {"\n"}
          {"\u2022"} زيادة الوعي المجتمعي حول مخاطر التحيز وأهمية الذكاء الاصطناعي الأخلاقي.
          {"\n"}
          {"\u2022"} دمج أخلاقيات الذكاء الاصطناعي في المناهج التعليمية والتدريبية.
          {"\n"}
          {"\u2022"} تطوير معايير دولية موحدة لمراجعة وتقييم الأنظمة الذكية.
          {"\n\n"}
          <Text style={styles.subSectionTitle}>توقعات مستقبلية:</Text>
          {"\n"}
          بحلول عام 2030، من المتوقع أن تصبح مراجعة التحيز جزءًا أساسيًا من عملية تطوير أي نظام ذكاء اصطناعي.
          {"\n"}
          كما يتوقع أن تظهر وظائف جديدة متخصصة في مراجعة أخلاقيات الذكاء الاصطناعي، مثل "مدقق أخلاقيات الذكاء الاصطناعي".
          {"\n\n"}
        </Text>

        <Text style={styles.quote}>
          "الخوارزميات ليست محايدة بطبيعتها، بل تعكس تحيزات من يصممها ويغذيها بالبيانات."{"\n"}
          — باحث في أخلاقيات الذكاء الاصطناعي
        </Text>

        <Text style={styles.sectionTitle}>8. الخاتمة</Text>
        <Text style={styles.content}>
          في الختام، يمثل التحيز في خوارزميات الذكاء الاصطناعي تحديًا كبيرًا يتطلب تعاونًا بين الباحثين والمطورين وصناع القرار. من الضروري تطوير آليات فعالة لرصد التحيز والحد منه، لضمان عدالة وشفافية الأنظمة الذكية في المستقبل.
          {"\n\n"}
          هذا البحث استعرض أهم الجوانب المتعلقة بالتحيز في الذكاء الاصطناعي، من المصادر إلى التأثيرات وطرق المعالجة. يبقى المجال بحاجة إلى مزيد من الدراسات والتجارب لضمان تطوير أنظمة أكثر عدالة وإنصافًا.
          {"\n\n"}
          إن توسعة النصوص في هذا المجال تتطلب الاستمرار في البحث، وتوثيق التجارب، ومشاركة النتائج مع المجتمع العلمي، لضمان تطوير أنظمة ذكاء اصطناعي أكثر عدالة وشفافية.
          {"\n\n"}
          <Text style={styles.subSectionTitle}>توصيات ختامية:</Text>
          {"\n"}
          {"\u2022"} الاستثمار في البحث العلمي متعدد التخصصات حول أخلاقيات الذكاء الاصطناعي.
          {"\n"}
          {"\u2022"} تعزيز الشفافية والمساءلة في تطوير ونشر الأنظمة الذكية.
          {"\n"}
          {"\u2022"} إشراك المجتمعات المتأثرة في تقييم الأنظمة الذكية.
          {"\n"}
          {"\u2022"} تطوير سياسات وتشريعات تضمن العدالة والإنصاف في الذكاء الاصطناعي.
          {"\n\n"}
        </Text>
      </View>
    </ScrollView>
  );
};

const styles = StyleSheet.create({
  container: {
    paddingVertical: 12,
    paddingHorizontal: 8,
    backgroundColor: '#fff',
    alignItems: 'center',
  },
  contentWrapper: {
    width: '100%',
    maxWidth: 480,
    alignSelf: 'center',
  },
  backButton: {
    alignSelf: 'flex-end',
    marginBottom: 8,
    marginRight: 2,
    padding: 6,
    borderRadius: 20,
    backgroundColor: '#f0f0f0',
  },
  title: {
    fontSize: 20,
    fontWeight: 'bold',
    marginBottom: 14,
    color: '#1a237e',
    writingDirection: 'rtl',
    textAlign: 'right',
  },
  sectionTitle: {
    fontSize: 17,
    fontWeight: 'bold',
    color: '#3949ab',
    marginTop: 14,
    marginBottom: 6,
    writingDirection: 'rtl',
    textAlign: 'right',
  },
  subSectionTitle: {
    fontSize: 15,
    fontWeight: 'bold',
    color: '#1976d2',
    marginTop: 8,
    marginBottom: 4,
    writingDirection: 'rtl',
    textAlign: 'right',
  },
  image: {
    width: SCREEN_WIDTH - 32 > 480 ? 448 : SCREEN_WIDTH - 32,
    height: (SCREEN_WIDTH - 32 > 480 ? 448 : SCREEN_WIDTH - 32) * 0.5,
    borderRadius: 10,
    marginBottom: 14,
    alignSelf: 'center',
  },
  content: {
    fontSize: 14.5,
    color: '#333',
    lineHeight: 24,
    writingDirection: 'rtl',
    textAlign: 'right',
    marginBottom: 6,
  },
  quote: {
    fontSize: 14,
    color: '#616161',
    fontStyle: 'italic',
    backgroundColor: '#f5f5f5',
    borderRightWidth: 3,
    borderRightColor: '#3949ab',
    padding: 10,
    marginVertical: 12,
    writingDirection: 'rtl',
    textAlign: 'right',
    borderRadius: 7,
  },
});

export default AIBiasInAlgorithmsArticle;
